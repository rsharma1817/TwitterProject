{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter API 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPchFACzHZPurC9jLbsURf4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsharma1817/TwitterProject/blob/main/Twitter_API_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxfKMsPzSl5A"
      },
      "source": [
        "!pip install tweepy\n",
        "import tweepy\n",
        "\n",
        "!pip install searchtweets\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import requests_oauthlib\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DIR = r'/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "keyFile = open(DIR + '/keys.txt', 'r')\n",
        "consumer_key = keyFile.readline().rstrip()\n",
        "consumer_secret = keyFile.readline().rstrip()\n",
        "access_token = keyFile.readline().rstrip()\n",
        "access_token_secret = keyFile.readline().rstrip()\n",
        "bearer_token = keyFile.readline().rstrip()\n",
        "keyFile.close()\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT0-AmDOZInW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b172322a-ae22-4d4a-bd52-bc3d35d0d8ab"
      },
      "source": [
        "!curl --request GET 'https://api.twitter.com/2/tweets/search/recent?query=from:twitterdev' --header 'Authorization: Bearer AAAAAAAAAAAAAAAAAAAAAFCIOQEAAAAAb6gbAdiRNDHRIz1p5%2BdNm47at2c%3Do1ek5MhAAouqJUSxB1faPoOpurlOCmjK7BVrKpFwmESyN9w4GT'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"data\":[{\"id\":\"1415477739734265856\",\"text\":\"@i_am_daniele Nobody's perfect \\uD83D\\uDE42 Set a reminder here:\\n\\nhttps://t.co/PGcoLoYk0u\"},{\"id\":\"1415457498803232770\",\"text\":\"\\uD83D\\uDFE3 Spaces endpoints are coming to v2! For the next few weeks, @i_am_daniele and a few guests will host a series of Spaces to guide you to build the best Spaces integrations. Set a reminder and join in the conversation ⬇️\\n\\nhttps://t.co/ConE57mnaC\"},{\"id\":\"1415348607813832708\",\"text\":\"Retweet endpoints are now available on the #TwitterAPI v2, enabling developers to build new solutions for engaging in, and understanding, the public conversation.\\n\\nWe won't mind if you use this Tweet to practice with the POST method \\uD83D\\uDE09\\nhttps://t.co/ht0UqkdqbM\"},{\"id\":\"1413515358766452738\",\"text\":\"Starting in less than an hour! \\uD83D\\uDEA8\\n\\nJoin us on https://t.co/GrtBOXh5Y1 for a recap of our recent launches on the #TwitterAPI v2 and to learn how to get started with the Tweet counts endpoints in Python and R #rstats https://t.co/gLexFXzaD5\"}],\"meta\":{\"newest_id\":\"1415477739734265856\",\"oldest_id\":\"1413515358766452738\",\"result_count\":4}}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1b5vpa5eodZ"
      },
      "source": [
        "below is a script i copied from here: https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Full-Archive-Search/full-archive-search.py\n",
        "\n",
        "i think i get what each of these sub-functions is doing.\n",
        "\n",
        "i have changed what the \"main\" function does (compared to the original linked above) by returning instead of printing -- could that be the problem?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9cbsAz2SrTe"
      },
      "source": [
        "## changed the parameters to allow for looping through lists of dates and times\n",
        "def full_archive_search(start, end):\n",
        "  # To set your environment variables in your terminal run the following line:\n",
        "  # export 'BEARER_TOKEN'='<your_bearer_token>'\n",
        "  bearer_token = 'AAAAAAAAAAAAAAAAAAAAAFCIOQEAAAAAb6gbAdiRNDHRIz1p5%2BdNm47at2c%3Do1ek5MhAAouqJUSxB1faPoOpurlOCmjK7BVrKpFwmESyN9w4GT'\n",
        "\n",
        "  search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
        "\n",
        "  # Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
        "  # expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
        "  query_params = {'query': 'Iran nuclear',\n",
        "                  'expansions': 'author_id',\n",
        "                  'tweet.fields': 'author_id,' + 'conversation_id,' + 'created_at',\n",
        "                  'max_results': 500,\n",
        "                  'start_time': start,\n",
        "                  'end_time': end,\n",
        "                  'user.fields': 'verified,' + 'public_metrics'}\n",
        "\n",
        "\n",
        "  def create_headers(bearer_token):\n",
        "      headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "      return headers\n",
        "\n",
        "\n",
        "  def connect_to_endpoint(url, headers, params):\n",
        "      response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
        "      #print(response.status_code)\n",
        "      if response.status_code != 200:\n",
        "          raise Exception(response.status_code, response.text)\n",
        "      return response.json()\n",
        "\n",
        "  ## commented out lines are either the original print(json.dumps()) or lines I used for testing\n",
        "  def main():\n",
        "      headers = create_headers(bearer_token)\n",
        "      json_response = connect_to_endpoint(search_url, headers, query_params)\n",
        "      #return json_response\n",
        "      #print(json.dumps(json_response, indent=4, sort_keys=False))\n",
        "      #print(type(json_response))\n",
        "      return json_response\n",
        "\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      main()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH4rg46ffxIp"
      },
      "source": [
        "I still need to work on the looping below, but I have my lists. Every 30 minutes might be too long though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4qSpptUOzqX"
      },
      "source": [
        "times = ['00:00:00Z', '00:30:00Z', '01:00:00Z', '01:30:00Z', '02:00:00Z', '02:30:00Z', '03:00:00Z', '03:30:00Z', '04:00:00Z', \n",
        "         '04:30:00Z', '05:00:00Z', '05:30:00Z', '06:00:00Z', '06:30:00Z', '07:00:00Z', '07:30:00Z', '08:00:00Z', '08:30:00Z',\n",
        "         '09:00:00Z', '09:30:00Z', '10:00:00Z', '10:30:00Z', '11:00:00Z', '11:30:00Z', '12:00:00Z', '12:30:00Z', '13:00:00Z',\n",
        "         '13:30:00Z', '14:00:00Z', '14:30:00Z', '15:00:00Z', '15:30:00Z', '16:00:00Z', '16:30:00Z', '17:00:00Z', '17:30:00Z',\n",
        "         '18:00:00Z', '18:30:00Z', '19:00:00Z', '19:30:00Z', '20:00:00Z', '20:30:00Z', '21:00:00Z', '21:30:00Z', '22:00:00Z',\n",
        "         '22:30:00Z', '23:00:00Z', '23:30:00Z', '24:00:00Z']\n",
        "\n",
        "dates = ['2013-11-24T', '2014-01-20T', '2015-04-02T', '2015-07-14', '2015-07-20T', '2015-09-17T', '2016-01-16T', '2018-05-08T', '2019-07-01T,', '2019-09-08T']\n",
        "\n",
        "#for time in times:\n",
        " # try:\n",
        " #     statuses = statuses + api.search_full_archive('environment', \"Iran nuclear\" + \" lang:en\", fromDate = date + times[count], toDate = date + times[count + 1],  maxResults=100)\n",
        " #   except:\n",
        "#      statuses = api.search_full_archive('environment', \"Iran nuclear\" + \" lang:en\", fromDate = date + times[count], toDate = date + times[count + 1], maxResults = 100)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDUpG92LgG_q"
      },
      "source": [
        "Testing timestamps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4TUOMTogPfw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4960a7f5-57c2-4298-84ad-406c8c818e37"
      },
      "source": [
        "(dates[4] + times[23])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2015-07-20T11:30:00Z'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xLiCYagKER"
      },
      "source": [
        "The new JSON format in v2 vs. v1 might be messing me up: https://developer.twitter.com/en/docs/twitter-api/tweets/search/migrate/standard-to-twitter-api-v2\n",
        "\n",
        "But this should be creating a SearchResults object, I think. Which is just a JSON payload. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsQNBduYbjk"
      },
      "source": [
        "tweets = full_archive_search(dates[0] + times[0], dates[0] + times[1])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul-F3EgzgYmc"
      },
      "source": [
        "But I guess not!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwoSupdEzMi4",
        "outputId": "56dbdba5-b2ec-447c-c9c4-91f6f509c0fa"
      },
      "source": [
        "type(tweets)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pflu5uBcd8hk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a84a746e-2b48-457f-eeb2-44672cb4cf3a"
      },
      "source": [
        "df = pd.read_json(tweets)\n",
        "print(df)\n",
        "#df.to_csv('data.csv')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d4e272e8b745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#df.to_csv('data.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m     )\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'NoneType'>"
          ]
        }
      ]
    }
  ]
}